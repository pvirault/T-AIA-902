{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import gym\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment\n",
    "\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "# Initialize the q-table with zero values\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "qtable = np.zeros((state_size, action_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate  = 0.9  # Learning-rate\n",
    "discount_rate = 0.8   # Discount-factor\n",
    "\n",
    "epsilon = 1           # Exploration rate\n",
    "max_epsilon = 1.0     # Exploration probability at start\n",
    "min_epsilon = 0.01    # Minimum exploration probability \n",
    "decay_rate= 0.005     # Exponential decay rate for exploration prob\n",
    "\n",
    "# training variables\n",
    "total_episodes = 100000 # Total number of episodes\n",
    "max_steps = 100       # Max steps per episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/PAULINE/Desktop/MSC Pro/Semestre 10/T-AIA-902-TLS_1/env/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "# Perform 100,000 episodes\n",
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()[0]\n",
    "    terminated = False\n",
    "\n",
    "    # print(f\"step: {episode} out of {total_episodes}\")\n",
    "\n",
    "    # Decrease epsilon\n",
    "    epsilon = np.exp(-decay_rate*episode) # Update epsilon\n",
    "    \n",
    "    # Loop \n",
    "    for step in range(max_steps):\n",
    "\n",
    "        # exploration-exploitation tradeoff\n",
    "        if random.uniform(0,1) < epsilon:\n",
    "            # explore\n",
    "            action = env.action_space.sample() # Explore the action space\n",
    "        else:\n",
    "            # exploit\n",
    "            action = np.argmax(qtable[state,:]) # Exploit learned values\n",
    "\n",
    "        # Apply the action and see what happens, observe reward\n",
    "        new_state, reward, terminated, truncated, info = env.step(action) \n",
    "        \n",
    "        current_value = qtable[state, action]  # current Q-value for the state/action couple\n",
    "        next_max = np.max(qtable[new_state])  # next best Q-value\n",
    "        \n",
    "        # Compute the new Q-value with the Bellman equation\n",
    "        qtable[state, action] = current_value + learning_rate * (reward + discount_rate * next_max - current_value )\n",
    "\n",
    "        # Update our current state\n",
    "        state = new_state\n",
    "    \n",
    "        # If terminated (if we're dead) : finish episode\n",
    "        if terminated == True: \n",
    "            break\n",
    "\n",
    "    # Update epsilon\n",
    "    epsilon = max(epsilon - decay_rate, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed over 100000 episodes\n",
      "Q-table after training:\n",
      "[[  0.           0.           0.           0.           0.\n",
      "    0.        ]\n",
      " [ -3.78665092  -2.32697494  -3.77265608  -3.65840259  -1.6445568\n",
      "  -11.31795219]\n",
      " [  0.20423483   1.53692134  -1.85451165   1.08309372   3.192\n",
      "   -7.49618211]\n",
      " ...\n",
      " [ -2.3097276   -1.7983998   -2.24316     -2.3089896  -11.07936\n",
      "  -11.36955744]\n",
      " [ -3.81037232  -3.57229073  -3.80735958  -3.84037219  -9.99\n",
      "  -12.49217777]\n",
      " [ -1.7118       7.7998847    4.2566853   -0.999       -9.\n",
      "   -9.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training completed over {total_episodes} episodes\")\n",
    "print('Q-table after training:')\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Step: 12.36\n",
      "Success rate: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "# env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "env.reset()\n",
    "\n",
    "episodes = 100\n",
    "success_rate = []\n",
    "steps = []\n",
    "\n",
    "# Evaluation\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()[0]\n",
    "    step = 0\n",
    "    terminated = False\n",
    "    \n",
    "    # Until the agent gets stuck or reaches the goal, keep training it\n",
    "    for step in range(max_steps):\n",
    "        # Choose the action with the highest value in the current state\n",
    "        action = np.argmax(qtable[state,:])\n",
    "\n",
    "        # Implement this action and move the agent in the desired direction\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        if terminated:\n",
    "            success_rate.append(int(reward == 20))\n",
    "            steps.append(step)\n",
    "            \n",
    "            break\n",
    "\n",
    "        # Update our current state\n",
    "        state = new_state\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Let's check our success rate!\n",
    "print(\"Mean Step:\", np.mean(steps))\n",
    "print(\"Success rate:\", np.mean(success_rate)*100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
